# RAG | AI-Sport-Support

Система поддержки спортсменов, тренеров и судей в фигурном катании на основе RAG (Retrieval-Augmented Generation).  
Проект дополняет телеграм-бота возможностью отвечать на вопросы по актуальным официальным правилам: судейство, элементы, техника исполнения, стоимость элементов, регламенты и порядок проведения соревнований и т.д.

---

## Цель проекта

Создать и интегрировать в телеграм-бота RAG-систему, которая:

- извлекает знания из официальных документов (русский и английский язык),
- отвечает на вопросы на естественном языке,
- использует только актуальные правила фигурного катания.

**Ожидаемый результат в рамках курса:**

- реализована RAG-система по правилам фигурного катания;
- система интегрирована в телеграм-бота;
- пользователи (спортсмены, тренеры, судьи) получают ответы на основе официальных документов.

---

## Основные задачи

- Извлечение данных из PDF и веб-страниц.
- Корректное выделение и обработка таблиц.
- Нормализация и очистка текста.
- Приведение таблиц к текстовому виду, удобному для embedding-моделей.
- Разбиение текста на чанки.
- Создание векторной базы (FAISS).
- Поддержка RAG-поиска по правилам на русском и английском языках.
- Тестирование качества (бенчмарки).
- Интеграция в телеграм-бота.

---

## Сбор данных

### Источники

- **Профессионалы** – официальные правила на сайте Федерации фигурного катания России  
  `http://ffssr.ru/page/docsrules.html` (PDF)
- **Любители** – документы АРМФК 
  `https://armfk.ru/documents` (PDF)
- **ISU** – международные правила на английском языке  
  - стартовая веб-страница:  
    `https://www.isu.org/figure-skating-rules/?tab=ISU%20Judging%20System`
  - связанные PDF-файлы с правилами и документами

PDF-файлы были отобраны и скачаны вручную, чтобы:

- включить только актуальные и релевантные документы;
- исключить лишние файлы и нерелевантную информацию.

### Объём данных

В сумме использовано:

- 28 PDF-файлов с правилами;
- 1 веб-ресурс (страница ISU).

Структура файлов:

```text
pdf_rules/
├── Профессионалы/           # 8 файлов
│   └── ... .pdf
├── Любители/                # 6 файлов
│   └── ... .pdf
└── ISU/                     # 14 файлов
    └── ... .pdf
# Веб-страница:
url_isu = "https://www.isu.org/figure-skating-rules/?tab=ISU%20Judging%20System"
```

---

## Архитектура проекта

```text
├── dataloader.py          # Функции загрузки и обработки PDF и веб-ресурсов
├── pdf_rules/             # Исходные PDF с правилами
│   ├── Профессионалы/
│   ├── Любители/
│   └── ISU/
├── rules_url.py           # Ссылки на веб-страницы с правилами
├── extract_pdf/           # (создаётся) очищенный текст по страницам для проверки
├── faiss_index_bge_m3/    # (создаётся) сохранённый FAISS-индекс
├── chunks.pkl             # (создаётся) список чанков после разбиения
└── README.md              # Документация
```

---

## Обработка веб-ресурсов

Для загрузки и парсинга веб-страниц используется `WebBaseLoader` (LangChain).

Основные шаги:

1. Загрузка HTML-контента страницы ISU.
2. Извлечение текста.
3. Дополнительная постобработка:  
   разбиение «склеенных» слов на границах заглавных букв, например:

**Пример:**
> `SustainabilityPressAnti-dopingSafeguardingISU` →  
> `Sustainability Press Anti-doping Safeguarding ISU`

---

## Обработка PDF

PDF-документы содержат большое количество сложных таблиц:

- объединённые ячейки;
- перенос таблиц на следующую страницу без повторения заголовков;
- чередование текста и таблиц в произвольном порядке.

Простые загрузчики (`PDFLoader`, `UnstructuredPDFLoader` в базовом виде) не дают корректного извлечения таких таблиц, поэтому реализован отдельный пайплайн.

### Шаги обработки PDF

1. **Извлечение текста**  
   - Постранично, с помощью `UnstructuredPDFLoader (mode="paged")`.
   - Сохранение метаданных для каждой страницы (номер, источник и т.д.).

2. **Извлечение таблиц**  
   - Используется `camelot.read_pdf(..., flavor="lattice")`.
   - Таблицы читаются по всем страницам документа.

3. **Объединение многостраничных таблиц**

   Таблица может быть разбита на несколько страниц. Чтобы восстановить логическую таблицу, система считает, что таблица на следующей странице — это продолжение предыдущей, если:

   - совпадает количество колонок;
   - страница идёт сразу следом: `page == prev_page + 1`.

   При этом:

   - при обнаружении повторяющегося заголовка на новой странице он удаляется;
   - таблица собирается в единый объект, а список страниц сохраняется как метаданные.

4. **Нормализация и конвертация таблиц в текст**

   Для каждой объединённой таблицы:

   - первая строка используется как заголовок (имена колонок);
   - пустые ячейки «дотягиваются» значениями из предыдущих строк (учёт объединённых ячеек);
   - полностью пустые строки и столбцы удаляются;
   - далее каждая строка таблицы превращается в текст вида:

     ```text
     Колонка 1: значение 1; Колонка 2: значение 1; ...; Колонка n: значение 1
     Колонка 1: значение 2; Колонка 2: значение 2; ...; Колонка n: значение 2
     ...
     Колонка 1: значение m; Колонка 2: значение m; ...; Колонка n: значение m
     ```

   Такой формат хорошо подходит для embedding-моделей и сохраняет связи между значениями в строке таблицы.

5. **Очистка текста**

   Для итогового текста, собранного со страницы, выполняется фильтрация «шумовых» строк. Система удаляет:

   - однотипные номера страниц;
   - пустые и бессодержательные строки;
   - технические артефакты (`dtype`, `Series`, служебные надписи библиотек);
   - отдельные буквы и обрывки таблиц;
   - строки, состоящие только из пунктуации или коротких числовых паттернов.

6. **Формирование финальных документов**

   Для каждой страницы PDF:

   - берётся очищенный текст страницы;
   - к нему добавляются все таблицы, которые **начинаются** на этой странице, в текстовом виде;
   - формируется единый текстовый блок, соответствующий странице;
   - сохраняются метаданные:
     - номер страницы,
     - источник (путь к PDF-файлу),
     - дополнительная информация (при необходимости).

   Результат — список объектов `Document`, пригодных для дальнейшего разбиения на чанки и векторизации.

---

## Подготовка к RAG

После того как все веб- и PDF-документы обработаны и объединены:

1. **Разбиение на чанки**

   - Используется `RecursiveCharacterTextSplitter`.
   - Основные параметры:
     - `chunk_size = 1000` символов,
     - `chunk_overlap = 200` символов,
     - разделители: `["\n\n\n", "\n\n", "\n", " "]`.


2. **Векторизация**

   - Мультиязычная модель эмбеддингов: `BAAI/bge-m3`.
   - Поддерживает русский и английский языки.

3. **Создание векторной базы**

   - Используется FAISS через `langchain_community.vectorstores.FAISS`.
   - Метрика: косинусное расстояние (`DistanceStrategy.COSINE`).
   - Индекс сохраняется локально в директорию `faiss_index_bge_m3/`.

4. **RAG-поиск**

   На основе сохранённого индекса:

   - создаётся `retriever` с типом поиска `similarity`;
   - по текстовому запросу пользователя находятся `k = 5` наиболее релевантных чанков;